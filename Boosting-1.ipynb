{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3afd6af3-6163-4a50-8113-55a8bc3169ed",
   "metadata": {},
   "source": [
    "Boosting is a machine learning ensemble technique where multiple weak learners are combined to create a strong learner.Weak learners are typically simple models that perform slightly better than random guessing such as derscsion tree with a limited depth.Boosting trains these weak learners sequentially,with each new model focusing or the instances that previous models misclassfied therby improving the overall performance of the ensemble popularr boosting algorthims include adaboost,gradient boosting machine(GBM),and xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e2a85a-3042-4f7d-8eec-24185e4bece2",
   "metadata": {},
   "source": [
    "Advantage of usingg boosting techniques:\n",
    "1. Improved accuracy\n",
    "2. Robustness to overfitting\n",
    "3. Flexiblity\n",
    "4. Feature importance\n",
    "5. Handling imblance data\n",
    "Limitaions of using boosting techniques\n",
    "1. senstivity to noisy data\n",
    "2. compiutationally intensive\n",
    "3. Hyperparamter tuning\n",
    "4. Limited interpretablity\n",
    "5. Vulnerablity to data distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f46d6a-3be3-424f-9180-01a0fd79cc90",
   "metadata": {},
   "source": [
    "Boosting works by squentially training a series of weak learners,where each subsquent learner focuse on the instances that were misclassfied or poorly predcited by the previous ones.The process can be summarized in sevaral steps\n",
    "1. Intialization\n",
    "2. weighted Training\n",
    "3. Iterative Training\n",
    "4. Combination\n",
    "5. Final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2ed9ed-c370-40d3-b4e3-f823a001ec83",
   "metadata": {},
   "source": [
    "There are sevral types of boosting algorthims each its own variation and charterstics spme of the most popular one include:\n",
    "1. AdaBoost(Adapative Boosting)\n",
    "2. Gradient Boosting Machine(GBM)\n",
    "3. XGBoost\n",
    "4. LightGBM\n",
    "5. CatBoost\n",
    "6. AdaBoost\n",
    "these are the few exampeles of boosting alogrthims and there are many other are many other variant implmentationavilable "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b71b40-b2f4-433a-8644-5276913ba312",
   "metadata": {},
   "source": [
    "Common parmeters in boosting algorthim include:\n",
    "1. Number of trees\n",
    "2. Learningg rate\n",
    "3. Tree Depth\n",
    "4. Tree Complexblity\n",
    "5. Subsample Ratio\n",
    "6. Column subsamppling \n",
    "7. Regularization\n",
    "8. Early stopping\n",
    "These paramters may vary sligtly depending on the specfic boostiing algorthim being used as adaboost gradient catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38edf7e4-27af-4066-86e4-f0a010d644f8",
   "metadata": {},
   "source": [
    "Boosting algorthim combine weak learners to create a strong learner though iterative training the genral outline how the processs works\n",
    "1. Intialization:intially each data point is assigned an equal weight\n",
    "2. Iterative Traning: Boosting algorthims train weak learners sequentially in each iteration the algorthimfocuses on sample that were misclassfied or had higher error in the previous iteration\n",
    "3. weighted Traning: at each iteration the algorthim adjust the weight of miscassfied samples giving higherr weight to those that were misclassfied \n",
    "4. Combining weak learner\n",
    "5. Final predction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b83a2fc-527d-4d62-9b15-a5e8c366dfa0",
   "metadata": {},
   "source": [
    "ADaboost short for adapative boosting is a popular ensemble learning algorthims used for classfication task it combine multiple weak learners often decsion the core concpet of adaboost revolves around itertively adjusting the weight of classfied data points,therby focusing more on the doiffcult instances in each iteration\n",
    "1. Intialzation\n",
    "2. Train weak learner\n",
    "3. Weighted voting\n",
    "4. update weight\n",
    "5. Iterative tarning \n",
    "6. Combine weak learners\n",
    "7. Final predction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e09c13-31db-499b-86db-08f985e0e736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
